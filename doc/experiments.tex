\section{Experiments}
We will evaluate our proposed method on the Atari games such as MsPacman, Freeway or Pong. We will train our agents on our adversary-resilient versions of A3C and DQN algorithms. We intend to use the fast gradient sign method \cite{seventeen} for generating adversarial examples. However, if possible, we will also test against the method proposed by \cite{eighteen}, which has been shown to resist several anti-adversarial attack methods. Since our focus in this paper is to mainly handle strategically-timed attacks, we will implement the method suggested by \cite{fourteen} to decide when to attack the agent. Following the pattern from previous papers on adversarial attacks on policy gradients \cite{thirteen, fourteen}, we will use average return as the metric for comparing the effect of the attack on the agent. Here, average return is the average cumulative reward across ten rollouts of the target policy. An agent is resistant to the adversary if its average reward remains relatively unchanged even in the presence of the attack. 

We will run experiments and compare the average return both in the presence and absence of strategically-timed attacks on an agent trained using the proposed algorithm under the following scenarios:
\begin{itemize}
\item  Compare performance for each of the following cases when an action is predicted using: 
\begin{itemize}
\item Both the input and predicted state at every time-step.
\item Only the predicted state when we believe an adversary may attack while using the input state all other times.
\item Using both the input and predicted state when we believe an adversary may attack while using the input state all other times.  
\end{itemize}
By doing this, not only can we judge if our proposed algorithm is effective in dealing with adversarial attacks but can also evaluate which setting is most resistant. In addition to this, since the video generation pipeline is expected to increase the computational load, therefore, by modifying the frequency at which we use its input we can make trade-offs between computation and average rewards.
\item Evaluate performance against the average portion of time steps in an episode that an adversary attacks the agent. This is referred to as the attack rate which can be tuned by changing the threshold value for strategically-timed attack. A lower threshold means a higher attack rate. 
\item Measure performance for different values of $\epsilon$ in FGSM adversarial examples. A small value of $\epsilon$ corresponds to a small amount of perturbation to the input. This will help us understand the limit of our method since the greater the perturbation, the more we must rely on our video prediction model for sampling actions.
\item A simple baseline model can be designed by using adversarial examples while training the agent which acts as a regularizer. An important experiment will therefore be to compare the performance of our proposed method with the baseline under similar attack conditions. 
\item Compare which of the two modified algorithms DQN and A3C are more resistant to adversarial inputs and under which conditions. 
\item If time permits, we will also test our method for multiple games as it will be interesting to see for which game our proposed method works the best. We hypothesize that in games such as Seaquest in which there are many random and new objects, the predicted frames will be less reliable and hence may show comparatively poor performance. 
\end{itemize}

