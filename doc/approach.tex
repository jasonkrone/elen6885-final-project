\section{Approach}

\subsection{Preliminaries}
In this work we assume that the adversarial attack against the agent
is constructed using a strategically timed attack as put forward by
Lin et al. (2017). This attack modifies the image $x$ input to a neural network
$f$ by solving this optimization problem:

$$\min_{\delta}{D_I(x, x + \delta)}$$
$$\mbox{subject to} f(x) \neq f(x + \delta)$$

where $D_I$ is an image similarity metric.
In our experiments, $x$ is a sequence of four atari game video frames
and $f$ is a Q network or policy network. The attack is carried out when
the output $f(x)$ indicates that the agent has a large preference for
taking a particular action $a^\star$ because this indicates that $a^\star$
will likely lead to large return and taking any other action will lead to
low return. The attacker can exploit this fact by perturbing $x$ by a small
amount $\delta$ such $f(x) \neq f(x + \delta)$ i.e. such that the agent no longer
takes action $a^\star$.

\subsection{Detection}
To detect a strategically timed attack we predict what we expect the state $s = x$
to be at a given time step and look for large discrepancies in the
action distribution for the predicted state and the "actual" state.
We predict the state $\hat s_t$ we expect to be input to $f$ at time step $t$
using the video prediction network $g$ put forward by Oh et al. (2015)
conditioned on the history $h_t = \{s_{t-1}, a_{t-2}, \dots, a_0, s_0\}$ of
previous frames and actions. Then to determine if there is an attack
at time step $t$, we compare the difference in action distributions
produced by $f(s_t)$ and $f(\hat st_t)$ using the \textrm{KL} divergence.
In summary, our procedure is as follows for each time step $t$:
\begin{enumerate}
    \item predict the expected state $\hat s_t = g(h_t)$
    \item compute policies for predicted state and current state $\hat \pi( \cdot \vert \hat s_t) = f(\hat s_t)$, $\pi( \cdot \vert s_t) = f(s_t)$
    \item determine that there is an attack if $\KL{\hat \pi( \cdot \vert \hat s_t)}{\pi( \cdot \vert s_t)} > c$
\end{enumerate}
where $c$ is a tuned threshold value. We will evaluate the success of a number of methods for selecting action $a_t$
based on $\hat \pi()$ and $\pi()$. Further details are given in the experiments section.

%\begin{itemize}
%    \item Take action $a = \argmax \hat \pi( \cdot \vert \hat s_t)$
%    \item Sample $a$ from $\hat \pi( \cdot \vert \hat s_t)$ using the given probabilites.
%    \item Take action $a =  \argmax \{a \times \hat \pi( \cdot \vert \hat s_t) + b \times \pi( \cdot \vert s_t) \}$
%\end{itemize}
%where $a$ and $b$ are tuned on multiple rollouts of the model. 
