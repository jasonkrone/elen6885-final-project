\begin{abstract}
Transfer learning has been extremely successful in the computer vision domain.
This exemplified by the fact that most state-of-the-art deep classification,
detection, and segmentation networks are first pre-trained on the imagenet dataset
and then fine tuned on the evaluation dataset. However, in the Reinforcement Learning domain,
transfer learning has not exhibited the same level of success. Policies learned by agents
are not easily transferable to new similar tasks. Moreover, most research in this area
has been confined to discrete action spaces. In this work, we benchmark and explore variants
of existing policy transfer methods on continuous control tasks, which until now have primarily been applied to
to discrete action spaces.
\end{abstract}
