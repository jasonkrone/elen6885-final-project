\section{Related Work}

Atari 2600 games provide challenging environments for Reinforcement Learning(RL) because of high-dimensional visual observations, partial observability, and delayed rewards. Approaches that combine deep learning and RL have made significant advances [\cite{one, two, three}]. Specifically, DQN [\cite{one}] combined Q-learning [\cite{four}] with a convolutional neural network (CNN) and achieved state-of-the-art performance on many Atari games. 

Although there has been several attempts to modeling videos, building a generative model that predicts the future frame is still a very challenging problem because it often involves high-dimensional natural-scene data with complex temporal dynamics. Thus, recent studies have mostly focused on modeling simple video data, such as bouncing balls or small patches, where the next frame is highly-predictable given the previous frames [\cite{fifteen, sixteen}]. In many applications, however, future frames depend not only on previous frames but also on control or action variable. \cite{three} used the Arcade Learning Environment (ALE) emulator for making action-conditional predictions with a Monte-Carlo tree search method [\cite{five}], to generate training data for a fast-acting CNN, which outperformed DQN on several domains. Our approach is mainly motivated by the idea of building a predictive model for vision-based RL problems introduced by \cite{six}. They proposed a neural network that predicts the attention region given the previous frame and an attention-guiding action. Building on the above, \cite{seven} proposed a recurrent neural network with multiplicative interactions that predicts the physical coordinate of a robot. In our problem statement, as Atari games, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, \cite{eight} introduces two neural network variants, which solves the above issues. Our frame prediction model is based on \cite{eight}.

Adversarial attacks on DNN's has been explored largely in the last few years, following \cite{nine} several methods for attacking DNN's have been proposed. \cite{twelve} estimated linear decision boundaries between classes of a DNN in the image space and iteratively shifted an image toward the closest of these boundaries for crafting an adversarial example. Majority of the proposed methods generated an adversarial example via seeking a minimal perturbation of an image that can confuse the classifier [\cite{ten, eleven}]. 

Adversarial attacks on deep RL agents has been recently established, \cite{thirteen} proposes uniform attack, which attacks a deep RL agent with adversarial examples at every time step in an episode for reducing the reward of the agent. \cite{fourteen} introduces strategically timed attack which is similar to the uniform attack proposed in \cite{thirteen} but achieves same effect by attacking four times less often on average. \cite{fourteen} also introduces enchanting attack which claims to be the first planning based adversarial attack to misguide the agent toward a target state. Since RL agents can be seen as being deployed to carry out certain high risk tasks such as autonomous driving, it is important to design algorithms that are able to handle adversarial attacks in order to reduce the chances of mishaps. With our project, we aim to explore one such strategy towards achieving this goal. 

